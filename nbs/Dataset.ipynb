{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "54049931-3d56-4a2b-8a6d-054c996714c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c1bfe42a-7a44-481b-a45f-bb6af64aed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch, torchvision, itertools, numpy as np, random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "89990d9c-a293-404d-b7e2-87bc2423a69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1732a009-689f-4255-a03f-b467b7a87a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../kin6-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "457bcabd-35b6-43c4-bc7e-3ead2c17409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_video_paths(file_path):\n",
    "    \"\"\"\n",
    "    Retrieves all videos from path and separates each subdirectory to its own class label\n",
    "\n",
    "    Args:\n",
    "        file_path (string): string path of desired directory\n",
    "\n",
    "    Returns:\n",
    "        video_paths [str]: list of string paths to video data.\n",
    "        labels [int]: list of labels as integers\n",
    "    \"\"\"\n",
    "    p = Path(file_path)\n",
    "    video_paths = []\n",
    "    labels = []\n",
    "    sub_dirs = [dir for dir in p.iterdir() if dir.is_dir()]\n",
    "    for i,sub_dir in enumerate(sub_dirs):\n",
    "        paths = [str(p) for p in sub_dir.iterdir()]\n",
    "        labels.extend([i for _ in range(len(paths))])\n",
    "        video_paths.extend(paths)\n",
    "    return video_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9af8ae6f-8086-413f-a195-11693bb2099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths, labels = get_video_paths('../kin6/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4b8a1ae5-8ddd-44ca-8ae4-75b5c6bc2ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3000)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video_paths), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753cbe2a-3681-4f10-92e3-14246a96fd39",
   "metadata": {},
   "source": [
    "`get_frame_indices` currently drops remaining part of video by default if there are less frames remaining than clip_length.\n",
    "\n",
    "Can be configured to return clips of less than determined clip length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e90ea199-489d-40bb-a5b5-578e80121899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "swin_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Resize((256,), interpolation=v2.functional.InterpolationMode.BILINEAR),\n",
    "    v2.CenterCrop(224),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5cc061b9-95dc-4eea-95e1-992efcf6c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_clip(clip):\n",
    "    \"\"\"\n",
    "    Formats clip based on Pytorch SwinTranformer3D inference example \n",
    "    https://pytorch.org/vision/0.18/models/generated/torchvision.models.video.swin3d_s.html#torchvision.models.video.swin3d_s\n",
    "\n",
    "    Args:\n",
    "        clip (tensor): clip of TCHW dimensions\n",
    "\n",
    "    Returns:\n",
    "        formatted_clip (tensor): dimensions CTHW\n",
    "    \"\"\"\n",
    "    formatted_frames = []\n",
    "    # Transforming clips of dimensions TCHW\n",
    "    \n",
    "    for frame in torch.unbind(clip):\n",
    "        formatted_frames.append(swin_transforms(frame))\n",
    "    # Swin3D transformer requires channels first CTHW\n",
    "    return torch.stack(formatted_frames).permute(1,0,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b187d3d-13a7-4c3c-a271-21a4d01f772f",
   "metadata": {},
   "source": [
    "Create CustomDataset that handles transformations and pass onto DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4fd3e241-d12d-48ef-8d5f-77581327cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_clip(video_path, clip_length=32):\n",
    "    \"\"\"\n",
    "    Reads video and return random batch of clip_length\n",
    "\n",
    "    Args:\n",
    "        video_path (str): path to video file\n",
    "        clip_length (int): length of image clip\n",
    "\n",
    "    Returns:\n",
    "        clip (tensor): clip of captured frames (TCHW)\n",
    "    \"\"\"\n",
    "    frames, _, _ = torchvision.io.read_video(video_path, output_format='TCHW')\n",
    "    total_frames = len(frames)\n",
    "    if total_frames <= 32:\n",
    "        indices = torch.arange(0, total_frames)\n",
    "        clip = frames[indices]\n",
    "        # short clip is padded with zeros to achieve 32 frame length tensor\n",
    "        pad = (0, 0, 0, 0, 0, 0, 0, 32-clip.shape[0])\n",
    "        clip = F.pad(clip, pad, value=0)\n",
    "        \n",
    "    else:\n",
    "        random_start = int(np.random.randint(0, total_frames-clip_length))\n",
    "        indices = torch.arange(random_start, random_start+clip_length)\n",
    "        clip = frames[indices]\n",
    "    \n",
    "    \n",
    "    return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0dbb1085-c53a-44df-a2e2-c3a05868ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = torch.randn((17,3,224,224))\n",
    "pad = (0, 0, 0, 0, 0, 0, 0, 32-clip.shape[0])\n",
    "clip = F.pad(clip, pad, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "beb3c770-8092-46cb-a9d6-1b91af04ddf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "678d8534-6d34-4424-9e4a-403191b6f91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip = torch.randn((1,3,224,224))\n",
    "clip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "97272c8e-2dda-489a-afe3-143c517332c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = (0, 0, 0, 0, 0, 0, 0, 1)\n",
    "clip = F.pad(clip, pad, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a0667be9-2723-43ce-9f07-6e34478260b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fff74219-1d3f-4d8d-82a8-c177c2c77276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(np.random.randint(0, 300-32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "41bbd3c0-d188-4800-81c1-0d7a84f57e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 256, 454])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip = get_clip(video_paths[0], clip_length=32)\n",
    "clip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "606746fc-c5bb-4268-b3f8-3a50445106a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 32, 224, 224]), torch.float32)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = format_clip(clip)\n",
    "res.shape, res.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "611017ad-71fc-45dd-8d58-a34ae293b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CustomVideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, clip_length=32, drop_last=True):\n",
    "        self.video_paths, self.labels, self.clip_length, self.drop_last = video_paths, labels, clip_length, drop_last\n",
    "\n",
    "    def __len__(self): return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        clip = get_clip(video_path, self.clip_length)\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        clip = format_clip(clip)\n",
    "        return clip, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "87452386-d15e-4bae-8020-4feea4113d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CustomVideoDataset(video_paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ac4e7c9b-affd-4f71-9568-b181495b93e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "38147e4e-cdbb-4c5e-9f56-ec1b9b1d812c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 32, 224, 224]), tensor(0))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "37c76bf3-8c09-4513-95a7-ec5eddac7fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "994e7751-fd58-4e50-b241-859a2f4cb549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05c843-5b67-4d20-aff9-f11b4a3cfe00",
   "metadata": {},
   "source": [
    "Creating the custom DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ebb182-9e87-4ab5-af3f-c78d03e8237c",
   "metadata": {},
   "source": [
    "Function handling the whole data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fadf889e-036c-4fcb-810b-7b91bae2ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths, labels = get_video_paths('../kin6/test')\n",
    "test_ds = CustomVideoDataset(video_paths, labels)\n",
    "test_dl = DataLoader(test_ds, 4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8d5603d8-d6bf-475f-a816-d0fba2219683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0b702416-d510-44c1-9c2d-5fe34e885587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "70a19c43-3e4d-496d-bc1d-10f8e7b1c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def create_datasets(train_path, test_path, clip_length):\n",
    "    \"\"\"\n",
    "    Creates training and test datasets.\n",
    "\n",
    "    Args:\n",
    "        train_path (string): path to training data directory\n",
    "        test_path (string): path to testing data directory\n",
    "        clip_length (int): number of frames per clip\n",
    "        \n",
    "    Returns:\n",
    "        train_ds (Dataset): Pytorch dataset for training\n",
    "        test_ds (Dataset): Pytorch dataset for testing\n",
    "    \"\"\"\n",
    "    if not clip_length: clip_length=32\n",
    "    train_paths, train_labels = get_video_paths(train_path)\n",
    "    test_paths, test_labels = get_video_paths(test_path)\n",
    "    train_ds, test_ds = CustomVideoDataset(train_paths, train_labels, clip_length), CustomVideoDataset(test_paths, test_labels, clip_length)\n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "127d72ce-d511-4052-8590-3610aea94e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_dataloaders(train_path, test_path, clip_length, bs):\n",
    "    \"\"\"\n",
    "    Creates dataloaders from trainining and testing\n",
    "\n",
    "    Args:\n",
    "        train_path (string): path to train data\n",
    "        test_path (string): path to test data\n",
    "        clip_length (int): length of each clip taken from video\n",
    "        bs (int): batch size value\n",
    "\n",
    "    Returns: \n",
    "        train dataloader (DataLoader)\n",
    "        test dataloader (DataLoader)\n",
    "    \"\"\"\n",
    "    if not clip_length: clip_length=32\n",
    "    if not bs: bs = 4\n",
    "    train_paths, train_labels = get_video_paths(train_path)\n",
    "    test_paths, test_labels = get_video_paths(test_path)\n",
    "    train_ds, test_ds = CustomVideoDataset(train_paths, train_labels, clip_length), CustomVideoDataset(test_paths, test_labels, clip_length)\n",
    "    train_dl, test_dl = DataLoader(train_ds, batch_size=bs, shuffle=True), DataLoader(test_ds, batch_size=bs)\n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d80f18dc-139f-423f-a2c2-f2149cd2d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = create_dataloaders('../kin6/train', '../kin6/test', clip_length=32, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "49abe798-5ad6-414c-8ddb-0daf5bd3d39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 73)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3000 // 4, 292 // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7a5a1f04-f5b5-4433-906f-9a6eeeafef1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 73)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl), len(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1fb7e031-7087-4143-accc-9f23e8104ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 32, 224, 224])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3a8e138b-f2db-4b4c-b86b-d3c3b998af94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "471e2969-26f9-4896-a4b2-35686ed3e67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 32, 224, 224]), torch.float32)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(test_dl))\n",
    "xb.shape, xb.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "94b37d51-891d-4504-9d7f-55681cd9318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prepare_sim_data(train_path: str, test_path: str, clip_length: int, num_partitions: int, batch_size: int, val_ratio: float = 0.1):\n",
    "    \"\"\"This function partitions the training set into N disjoint\n",
    "    subsets, each will become the local dataset of a client. This\n",
    "    function also subsequently partitions each traininset partition\n",
    "    into train and validation. The test set is left intact and will\n",
    "    be used by the central server to asses the performance of the\n",
    "    global model.\n",
    "\n",
    "    Args:\n",
    "        train_path (str): Path to train data\n",
    "        test_path (str): Path to test data\n",
    "        clip_length (int): Number of frames per video clip\n",
    "        num_partitions (int): Number of partitions to create from original data\n",
    "        batch_size (int): DataLoader batch size\n",
    "        val_ratio (float): % of train data reserved for validation\n",
    "\n",
    "    Returns:\n",
    "        list of trainloaders [DataLoader]\n",
    "        list of validation loaders [DataLoader]\n",
    "        testloader (DataLoader)\n",
    "    \"\"\"\n",
    "\n",
    "    # get datasets\n",
    "    trainset, testset = create_datasets(train_path, test_path, clip_length)\n",
    "\n",
    "    # split trainset into `num_partitions` trainsets\n",
    "    num_images = len(trainset) // num_partitions\n",
    "\n",
    "    partition_len = [num_images] * num_partitions\n",
    "\n",
    "    trainsets = random_split(\n",
    "        trainset, partition_len, torch.Generator().manual_seed(2023)\n",
    "    )\n",
    "\n",
    "    # create dataloaders with train+val support\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for trainset_ in trainsets:\n",
    "        num_total = len(trainset_)\n",
    "        num_val = int(val_ratio * num_total)\n",
    "        num_train = num_total - num_val\n",
    "\n",
    "        for_train, for_val = random_split(\n",
    "            trainset_, [num_train, num_val], torch.Generator().manual_seed(2023)\n",
    "        )\n",
    "\n",
    "        trainloaders.append(\n",
    "            DataLoader(for_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        )\n",
    "        valloaders.append(\n",
    "            DataLoader(for_val, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        )\n",
    "\n",
    "    # create dataloader for the test set\n",
    "    testloader = DataLoader(testset, batch_size=batch_size)\n",
    "\n",
    "    return trainloaders, valloaders, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "69cf08e7-9e44-479e-8a4d-525a801b2a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloaders, valloaders, testloader = prepare_sim_data('../kin6/train', '../kin6/test', 32, 30, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6fafe26f-e425-4c4a-be8b-42d19df87885",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_partition = trainloaders[0].dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4c01fd25-b3ff-4a77-9a89-f55d69786b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of videos: 90\n"
     ]
    }
   ],
   "source": [
    "partition_indices = train_partition.indices\n",
    "print(f'number of videos: {len(partition_indices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf906b-ac69-48e3-a679-19448c33c22b",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c2cdd983-2af7-4ace-a5ea-aff213494905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
